---
title: "Project 2"
author: "Abdelmalek, Monu Chacko, Paul Perez"
date: "5/22/2021"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Project Problem Statement

***TODO:***

This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH. Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach. Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.

***TODO:***

# Executive Summary

***TODO:***
New Regulations by ABC beverage company leadership requires the company’s production unit to better understand the manufacturing process, the predictive factors and their relationship to the PH of the beverages.
***TODO:***

# Research Statement

***TODO:***
The research is an effort to find the predictive variables related to the ph of the beverages and build the predictive model for ph of beverages
***TODO:***

# Data Collection

***TODO:***
The data set is a historic data containing predictors associated to the PH and is provided in an excel file. We will utilize this historic dataset to analyze and predict the PH of beverages. Two excel files are provided:

- The training data (StudentData.xlsx)
- The test data (StudentEvaluation.xlsx).
***TODO:***

```{r install-packages, warning=FALSE, message=FALSE}
library("readxl")
library(httr)
library(caret)
library(tidyverse)
library(kableExtra)
library(caretEnsemble)
library(mice)
library(kableExtra)
library(xgboost)
library(dplyr)
library(parallel)
library(doParallel)
library(ggplot2)
library(Hmisc)
library(psych)
library(reshape2)
library(gridExtra)
library(rpart.plot)
library(DT)
library(earth)
#library(MASS)
#install.packages("TeachingDemos")
#install.packages("plotmo")

#Loading required package: lattice
#Loading required package: ggplot2
#Loading required package: class
#Loaded mda 0.5-2

#Loading required package: Formula
#Loading required package: plotmo
#Loading required package: plotrix
#Loading required package: TeachingDemos

#suppressWarnings(suppressMessages(library(data.table)))
#suppressWarnings(suppressMessages(library(openxlsx)))
#suppressWarnings(suppressMessages(library(psych)))
#suppressWarnings(suppressMessages(library(knitr)))
##suppressWarnings(suppressMessages(library(DataExplorer)))
#suppressWarnings(suppressMessages(library(VIM)))
#suppressWarnings(suppressMessages(library(corrplot)))
#suppressWarnings(suppressMessages(library(caret)))
#suppressWarnings(suppressMessages(library(PerformanceAnalytics)))
```


```{r install-packages-notes, warning=FALSE, message=FALSE, results='hide', echo=FALSE}
#install.packages("TeachingDemos")
#install.packages("plotmo")

#Packages required for earth
#Loading required package: lattice
#Loading required package: ggplot2
#Loading required package: class
#Loaded mda 0.5-2

#Loading required package: Formula
#Loading required package: plotmo
#Loading required package: plotrix
#Loading required package: TeachingDemos

# Other notes
#suppressWarnings(suppressMessages(library(data.table)))
#suppressWarnings(suppressMessages(library(openxlsx)))
#suppressWarnings(suppressMessages(library(psych)))
#suppressWarnings(suppressMessages(library(knitr)))
##suppressWarnings(suppressMessages(library(DataExplorer)))
#suppressWarnings(suppressMessages(library(VIM)))
#suppressWarnings(suppressMessages(library(corrplot)))
#suppressWarnings(suppressMessages(library(caret)))
#suppressWarnings(suppressMessages(library(PerformanceAnalytics)))
```


```{r warning=FALSE, message=FALSE}
se_data <- read.csv("https://raw.githubusercontent.com/monuchacko/cuny_msds/master/data_624/data/StudEval.csv")
sd_data <- read.csv("https://raw.githubusercontent.com/monuchacko/cuny_msds/master/data_624/data/StudData.csv")

```

```{r warning=FALSE, message=FALSE}
dim(sd_data)
```

```{r warning=FALSE, message=FALSE}
colnames(sd_data)
```

```{r warning=FALSE, message=FALSE}
str(sd_data)
```

```{r warning=FALSE, message=FALSE}
dim(se_data)
```

```{r warning=FALSE, message=FALSE}
colnames(se_data)
```

```{r warning=FALSE, message=FALSE}
str(se_data)
```




```{r warning=FALSE, message=FALSE}

#***TODO:***

#***TODO:***

bev_raw <- read_csv('https://raw.githubusercontent.com/monuchacko/cuny_msds/master/data_624/data/StudDataFile.csv') 
head(bev_raw)
```

# Data Preparation and EDA (Exploratory Data Analysis)

***TODO:***

***TODO:***


```{r warning=FALSE, message=FALSE}
bev_raw %>%
  ggplot(aes(PH, fill=PH > 9)) + 
  geom_histogram(bins=30) +
  theme_bw() +
  theme(legend.position='center') +
  labs(y='Count', title='PH Levels in Dataset')
```



```{r warning=FALSE, message=FALSE}
bev_raw <- bev_raw %>% 
  filter(!is.na(bev_raw$PH), bev_raw$PH < 9) 
```

```{r}
dim(bev_raw)
str(bev_raw)
hist.data.frame(bev_raw)
table(bev_raw$`Brand Code`)
summary(bev_raw)
describe(bev_raw %>% select(-`Brand Code`))
```

## Zero variance

***TODO:***
To filter for near-zero variance predictors, the caret package function nearZeroVar will return the column numbers of any predictors that fulfill the conditions outlined. A zero variance predictor will never be chosen for a split since it offers no possible predictive information.
***TODO:***

```{r warning=FALSE, message=FALSE}
df1 <- bev_raw %>% select(-`Brand Code`) %>% mutate_each(funs(as.numeric(.)))%>%complete.cases()%>%
       as.data.frame()


names(df1)[nearZeroVar(df1)]
```


```{r warning=FALSE, message=FALSE}
nzv <- nearZeroVar(bev_raw,saveMetrics= TRUE)
nzv[nzv$nzv,]
```

## Box plot

***TODO:***
Box plots for the variables reveal, that besides having the outliers in the variables, most of the variables are skewed. For example: Variables density, carb flow, filler speed and oxygen filler are skewed providing us an opportunity to further check their distribution.
***TODO:***



```{r warning=FALSE, message=FALSE}
df.m <- melt(bev_raw %>% select(-MFR, -`Filler Speed`, -`Carb Flow`,-`Bowl Setpoint`,`Carb Pressure1`,
            `Hyd Pressure4`, `Air Pressurer`, `Carb Temp`, `Filler Level`, `Mnf Flow`), id.var = "Brand Code")
p <-ggplot(data = df.m, aes(x=variable, y=value)) + 
  geom_boxplot(outlier.colour="red", outlier.shape=8, outlier.size=4,aes(fill=variable)) +     
  scale_y_continuous(name = "Predictors for PH", breaks = seq(0, 2, 0.5))  + 
  coord_flip()
p
```


```{r warning=FALSE, message=FALSE}

```




## Normality

***TODO:***
Normality is one of the most widely used technique to understand the continuous predictors. In the below plot we can see normal distribution behavior of the given dataset for different features.
***TODO:***

```{r warning=FALSE, message=FALSE}
bev_raw%>%
 select(-`Brand Code`) %>%   
  select(2:20) %>%         
  gather() %>%                            
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density() 
```



```{r warning=FALSE, message=FALSE}
bev_raw%>%
 select(-`Brand Code`) %>%   
  select(`Carb Flow`, Density, MFR, Balling, `Pressure Vacuum`, PH, `Oxygen Filler`, `Bowl Setpoint`, `Pressure Setpoint`, `Air Pressurer`, `Alch Rel`,`Carb Rel`,`Balling Lvl`)   %>%               
  gather() %>%                             
  ggplot(aes(value)) +                     
    facet_wrap(~ key, scales = "free") +  
    geom_density() 
```



# Data Preprocessing

***TODO:***
We perform 3 data preparation steps - Remove the near-zero variance variables we previously mentioned - Impute missing values with a Random Forest Regression with the MICE package - Create dummy variables for the categorical variable, brand

Random Forest was chosen as the regression method in imputation because it requires very little tuning, allowing and there is no interface to tune the imputation model in MICE.
***TODO:***


```{r data-process-1, warning=FALSE, message=FALSE}
cl <- makeCluster(detectCores())
registerDoParallel(cl)
set.seed(42)

bev <- select(bev_raw, -PH) %>%
  union_all(se_data) #since we're not using the labels (y values), combining the sets for data clearning is fine
cols <- str_replace(colnames(bev), '\\s', '')
colnames(bev) <- cols
bev <- mutate(bev, BrandCode = ifelse(is.na(BrandCode), 'Unknown', BrandCode))
y <- bev_raw$PH

zero_vars <- nearZeroVar(bev[1:2566, ])
bev_new <- bev[, -zero_vars]

pred <- mice(data = bev_new, m = 3, method = 'rf', maxit = 3)
bev_imputed <- complete(pred)

form <- dummyVars(~ ., data = bev_imputed)
bev_imputed <- predict(form, bev_imputed) %>% data.frame() %>% as.matrix()
```

# Model Building

## Test Train Split

***TODO:***
We will split the data into train/test sets before running cross validation. This will allow us to make sure that predictions are consistent at all levels of PH.
***TODO:***


```{r test-train-split-1, model-bld-1, warning=FALSE, message=FALSE}
bev_eval <- bev_imputed[2567:2833,]
bev_train <- bev_imputed[1:2566,]
samples <- createDataPartition(y, p = .75, list = F)
x_train <- bev_train[samples, ]
x_test<- bev_train[-samples, ]

y_train <- y[samples]
y_test <- y[-samples]
```


## Modeling Technique and Approach

***TODO:***
For state-of-the-art prediction quality, we will use a model stack. This will consist of tuning models separately and then combining the candidate models into a metamodel that will formulate predictions with a linear combination of our tuned models.

From the perspective of understanding the manufacturing process, the model stack will also provide benefits. The stack is like a panel of experts, each looking at the data through slightly different lenses to form their diagnoses. By looking at the predictors each model uses, we can gather assemble a complete picture of the factors that affect our manufacturing process.

Four of the five models will be tree-based ensembles because this model type can handle non-linear relationships, is robust to outliers and skewed distributions, and can model complex interactions. One non-tree modle is included for divsersity in the stack.
***TODO:***


```{r model-tech-app-1, warning=FALSE, message=FALSE}
xgb_grid <- expand.grid(eta = c(.01), nrounds = c(10), max_depth = c(6), gamma = c(0), colsample_bytree = c(.8), min_child_weight = c(.8), subsample = c(.8))
cub_grid <- expand.grid(.committees = c(5), .neighbors = c(7))
mars_grid <- expand.grid(.degree = c(2), .nprune = 24)
dart_grid <- expand.grid(eta = c(.01), nrounds = c(10), gamma = c(.1), skip_drop = c(.6), rate_drop = c(.4), max_depth = c(6), colsample_bytree = c(.6), min_child_weight = c(.6), subsample = c(.6))
rf_grid <- expand.grid(mtry = 25)

tuning_list <-list(
  caretModelSpec(method="xgbTree", tuneGrid = xgb_grid),
  caretModelSpec(method="xgbDART", tuneGrid = dart_grid),
  caretModelSpec(method="cubist", tuneGrid = cub_grid),
  caretModelSpec(method="rf", tuneGrid = rf_grid, importance = TRUE),
  caretModelSpec(method="bagEarth", tuneGrid = mars_grid)
)

my_control <- trainControl(method = 'cv',
                           number = 5,
                           savePredictions = 'final',
                           index = createFolds(y_train, 5),
                           allowParallel = TRUE) 

mod_list <- caretList(
  x = x_train,
  y = y_train,
  preProcess = c('center', 'scale'),
  trControl = my_control,
  tuneList = tuning_list
)
```

***TODO:***
The training grid is commented out because of the time required to run, but can be uncommented for reproducibility of our model optimization
***TODO:***

## MARS


```{r mars-1, warning=FALSE, message=FALSE}
#helper function
show_results <- function(model){ 
  rslts <- model$results %>%
    arrange(RMSE)
  head(rslts, 10) %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
}
```


```{r warning=FALSE, message=FALSE}
show_results(mod_list$bagEarth)
```


```{r warning=FALSE, message=FALSE}
predict(mod_list$bagEarth, x_test) %>%
  postResample(y_test)
```


***TODO:***

***TODO:***


## RandomForest


```{r randon-fr-1, warning=FALSE, message=FALSE}
show_results(mod_list$rf)
```


```{r warning=FALSE, message=FALSE}
predict(mod_list$rf, x_test) %>%
  postResample(y_test)
```



## Cubist


```{r cubist-1, warning=FALSE, message=FALSE}
show_results(mod_list$cubist)
```


```{r warning=FALSE, message=FALSE}
predict(mod_list$cubist, x_test) %>%
  postResample(y_test)
```

***TODO:***

***TODO:***


## XGB Trees

```{r warning=FALSE, message=FALSE}
show_results(mod_list$xgbTree)
```

```{r warning=FALSE, message=FALSE}
predict(mod_list$xgbTree, x_test) %>%
  postResample(y_test)
```

## XGB Dart


```{r warning=FALSE, message=FALSE}
show_results(mod_list$xgbDART)
```

```{r warning=FALSE, message=FALSE}
predict(mod_list$xgbDART, x_test) %>%
  postResample(y_test)
```

***TODO:***
Assessment

While the XGBTee model offers the lowest RMSE, it still only explains about 50% of the variance in PH. There is plenty of room for the other models to offer value. We will use the hyperparameters yielding the best RMSE in our final ensemble that will be built later.
***TODO:***


# Model Evaluation And Model Selection


```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```


***TODO:***

***TODO:***


## Variable Importances


```{r car-imp-1, warning=FALSE, message=FALSE}
p1 <- varImp(mod_list$cubist) %>%
  plot(top = 10, main = 'Cubist')
p2 <- varImp(mod_list$xgbTree) %>%
  plot(top = 10, main = 'XGBTrees')
p3 <- varImp(mod_list$xgbDART) %>%
  plot(top = 10, main = 'XGBDart')
p4 <- varImp(mod_list$bagEarth) %>%
  plot(top = 10, main = 'MARS')
p5 <- varImp(mod_list$rf) %>%
  plot(top = 10, main = 'RF')
grid.arrange(p1,p2,p3,p4,p5, nrow = 3, ncol = 2)
```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```

```{r warning=FALSE, message=FALSE}

```


***TODO:***
Observations:

The top predictor, MnFlow, is consistent throughout the set of models
The XGBoost models are similar in variables, but less similar in their distributions of importance
After the top 5, divergence increases
Based on what we see, there are more factors contributing to PH with the ensemble than with a single model. We’ll confirm this by looking at the count by predictor among each top 10:

Variable Counts among the Models
***TODO:***


```{r warning=FALSE, message=FALSE}
top_10 <- function(model) {
  varImp(model)$importance %>%
  rownames_to_column('var') %>%
  arrange(desc(Overall)) %>%
  head(10) %>%
  select(var) 
}

rbind(top_10(mod_list$xgbDART), top_10(mod_list$xgbTree), top_10(mod_list$cubist), top_10(mod_list$bagEarth), top_10(mod_list$rf)) %>%
  group_by(var) %>%
  summarise(ModelCount = n()) %>%
  arrange(desc(ModelCount)) %>%
  kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

***TODO:***
Best Single Tree

Because 4 of the 5 models in the ensemble are tree-based, looking a single decision tree model will give us an idea of not just what predictors are important, but how they are being used to form predictions.
***TODO:***

```{r warning=FALSE, message=FALSE}
rp_mod <- rpart::rpart(ph ~ ., data = data.frame(bev_train, ph = y), method = 'anova')
rpart.plot(rp_mod, uniform=TRUE)
```

## Test Model Stack

***TODO:***
Now that our models are optimized and understood, we will form the ensemble and test the results. We first look at their prediction correlations to confirm that the stack, intuitively, be effective. The lower the correlation between the models, the more effective the stack will be.
***TODO:***

```{r warning=FALSE, message=FALSE}
resamples <- resamples(mod_list)
modelCor(resamples) %>% kable() %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

***TODO:***
Our model list has negative correlations, indicating that it will be effective.
***TODO:***

```{r warning=FALSE, message=FALSE}
ensemble1 <- caretStack(
  mod_list,
  method="glmnet",
  metric="RMSE",
  trControl=trainControl(
    method="cv",
    number=5,
    savePredictions="final"
  )
)
ensemble1$ens_model$results %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

***TODO:***
We see a several percent improvement over the best single model.
***TODO:***

```{r warning=FALSE, message=FALSE}
preds <- predict(ensemble1, x_test) 
preds %>% postResample(y_test)
```

***TODO:***
Predicted Vs Actual
***TODO:***

```{r warning=FALSE, message=FALSE}
preds_df <- data.frame(predicted = preds, actual = y_test)
ggplot(preds_df) + geom_point(aes(x = predicted, y = actual)) + geom_smooth(aes(x = predicted, y = actual))
```


## Model Selection

***TODO:***
On the test set, the ensemble’s predictions are consistent throughout the range of PH. Consistent quality is, theoretically, a feature of ensemble models because the models are weighted such that predictions are maximized. If one model predicts better at lower PH, it will be upweighted over that range. There are several outliers, but when not all of the variance is explainable, this is unavoidable.
***TODO:***





## Predict on Eval Set


***TODO:***
With our hyperparameters of the individual models optimized, we want to provide each model the entire training set and build the meta-model based on these optimized single models.
***TODO:***


```{r warning=FALSE, message=FALSE}
xgb_grid <- expand.grid(eta = c(.01), nrounds = c(1000), max_depth = c(6), gamma = c(0), colsample_bytree = c(.8), min_child_weight = c(.8), subsample = c(.8))
cub_grid <- expand.grid(.committees = c(5), .neighbors = c(7))
mars_grid <- expand.grid(.degree = c(2), .nprune = 24)
dart_grid <- expand.grid(eta = c(.01), nrounds = c(1000), gamma = c(.1), skip_drop = c(.6), rate_drop = c(.4), max_depth = c(6), colsample_bytree = c(.6), min_child_weight = c(.6), subsample = c(.6))
rf_grid <- expand.grid(mtry = 25)

tuning_list <-list(
  caretModelSpec(method="xgbTree", tuneGrid = xgb_grid),
  caretModelSpec(method="xgbDART", tuneGrid = dart_grid),
  caretModelSpec(method="cubist", tuneGrid = cub_grid),
  caretModelSpec(method="rf", tuneGrid = rf_grid, importance = TRUE),
  caretModelSpec(method="bagEarth", tuneGrid = mars_grid)
)



my_control <- trainControl(method = 'cv',
                           number = 5,
                           savePredictions = 'final',
                           index = createFolds(y, 5),
                           allowParallel = TRUE) 

mod_list <- caretList(
  x = bev_train,
  y = y,
  preProcess = c('center', 'scale'),
  trControl = my_control,
  tuneList = tuning_list
)
final_ensemble <- caretStack(
  mod_list,
  method="glmnet",
  metric="RMSE",
  trControl=trainControl(
    method="cv",
    number=5,
    savePredictions="final"
  )
)
final_ensemble$ens_model$results %>% kable () %>% kable_styling(bootstrap_options = "striped", full_width = F)

```

***TODO:***
With more data available, the ensemble improve around 6%. Our model is considerably more predictive than a “baseline” single model.

Ensemble Model Importances
***TODO:***

```{r warning=FALSE, message=FALSE}
plot(varImp(final_ensemble$ens_model))
```

***TODO:***
The MARS model is excluded, but all four tree models contribute the the final prediction

Export Predictions
***TODO:***

```{r warning=FALSE, message=FALSE}
predictions <- predict(final_ensemble, bev_eval)
predictions %>% tibble::enframe(name = NULL) %>% datatable()
```

***TODO:***
The MARS model is excluded, but all four tree models contribute the the final prediction

Export Predictions
***TODO:***

```{r warning=FALSE, message=FALSE}
predictions <- predict(final_ensemble, bev_eval)
predictions %>% tibble::enframe(name = NULL) %>% datatable()
```


# Conclusion

***TODO:***
After working on extracting the data from the given files, we processed data cleansing and handle the missing values along with the NAs. We trained and tested the data 75% to 25% respectively. Our models were able to produce for us predicted values for pH which are also saved in predictions.csv file separately. We notice that all the values predicted are greater than 7 and more specifically greater than 8. This scale translates into saying that the beverage made is alkaline. At the beginning of this study, we were not informed about the nature of the ABC Beverage company, meaning of what type of beverage manufacturer it was. But from our studies we can conclude that this company produces alkaline beverages like water, dairy, tea, fruit drinks, etc.
***TODO:***


# References

***TODO:***
Github : https://github.com/monuchacko/cuny_msds/blob/master/data_624/project2.Rmd
RPUBS: 
Prediction Results: 
Forecasting: Principles and Practice - 2nd edition: https://otexts.com/fpp2/
Applied Predictive Modeling: https://github.com/hovig/Team5-Data624-Project2/blob/master/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf
***TODO:***





