---
title: "Data 622 Finding the best Model"
author: "Monu Chacko"
date: "12/6/2020"
output:
  html_document:
    toc: true
    toc_float: yes
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Part A

```{r warning=FALSE, message=FALSE} 
library(caret)
library(pROC)
library(tidyverse)
library(kableExtra)
library(ggplot2)
```

### STEP# 0: 

Pick any two classifiers of (SVM, Logistic, DecisionTree, NaiveBayes). Pick heart or ecoli dataset. Heart is simpler and ecoli compounds the problem as it is NOT a balanced dataset.

We pick the heart data

```{r warning=FALSE, message=FALSE}
data <- read.csv('https://raw.githubusercontent.com/monuchacko/cuny_msds/master/data_622/heart.csv',head=T,sep=',',stringsAsFactors=F, fileEncoding = "UTF-8-BOM")

# Check the column names
names(data)

# View sample data
head(data) %>% kable() %>% kable_styling() %>% scroll_box(width = "800px", height = "300px")
```

Clean and prepare data

```{r warning=FALSE, message=FALSE}
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)
data$ca <- as.factor(data$ca)
data$sex <- as.factor(data$sex)
data$restecg <- as.factor(data$restecg)
data$thal <- as.factor(data$thal)
data$target <- as.factor(data$target)
```

```{r warning=FALSE, message=FALSE}
colSums(is.na(data))
str(data)
summary(data)
head(data) %>% kable() %>% kable_styling() %>% scroll_box(width = "800px", height = "300px")
```

### STEP# 1 

Set a seed (43)

```{r warning=FALSE, message=FALSE}
# do a 80/20 split 
set.seed(43)
```

View Heart Disease vs Age Chart

```{r warning=FALSE, message=FALSE}
ggplot(data,aes(x=age,fill=target,color=target)) + geom_histogram(binwidth = 1,color="black") + labs(x = "Age",y = "Frequency", title = "Heart Disease vs Age")
```


### STEP# 2 

Do a 80/20 split and determine the Accuracy, AUC and as many metrics as returned by the Caret package (confusionMatrix). Call this the base_metric. Note down as best as you can development (engineering) cost as well as computing cost (elapsed time). 

Start with the original dataset and set a seed (43). Then run a cross validation of 5 and 10 of the model on the training set. Determine the same set of metrics and compare the cv_metrics with the base_metric. Note down as best as you can development (engineering) cost as well as computing cost(elapsed time). Start with the original dataset and set a seed (43) Then run a bootstrap of 200 resamples and compute the same set of metrics and for each of the two classifiers build a  three column table for each experiment (base, bootstrap, cross-validated). Note down as best as you can development (engineering) cost as well as computing cost(elapsed time).


```{r warning=FALSE, message=FALSE}
# 80/20 split 
train_ind <- sample(seq_len(nrow(data)), size = floor(0.8 * nrow(data)))
train_heart <- data[ train_ind,]
test_heart  <- data[-train_ind,]
```


```{r warning=FALSE, message=FALSE}
draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```

```{r warning=FALSE, message=FALSE}
eval_model <- function(train_method, tr, model_name){
  
  # Timer begin
  ptm <- proc.time()
  
  # set seed
  set.seed(43)
  
  # Base Model
  if (grepl("Base", model_name, fixed=TRUE)) {
    
    # train model
    dt_model = train(form = target ~ ., data = train_heart, trControl = tr, method = train_method)
    print(dt_model)
    
    # evaluate model
    model_cm <- confusionMatrix(predict(dt_model, subset(test_heart, select = -c(target))), test_heart$target)
    
    # Confusion Matrix visuals
    draw_confusion_matrix(model_cm)
    
    print(paste(model_name, 'results'))
    print(model_cm)
    
    # Timer end
    elapsed_time <- (proc.time() - ptm)[[3]]
  
    # determine the Accuracy, AUC and as many metrics as returned by the Caret package (confusionMatrix).
    # store results
    accuracy <- model_cm$overall[[1]]
    auc_val <- as.numeric(auc(roc(test_heart$target, factor(predict(dt_model, test_heart), ordered = TRUE))))
    sensitivity <- model_cm$byClass[[1]]
    specificity <- model_cm$byClass[[2]]
    precision <- model_cm$byClass[[5]]
    recall <- model_cm$byClass[[6]]
    f1 <- model_cm$byClass[[7]]
    
  # Bootstrap Model
  } 
  else if (grepl("Boot", model_name, fixed=TRUE)){
    
    dt_model = train(form = target ~ ., data = data, trControl = tr, method = train_method)
    
    # end timer
    elapsed_time <- (proc.time() - ptm)[[3]]
    
    accuracy <- c()
    auc_val <- c()
    sensitivity <- c()
    specificity <- c()
    precision <- c()
    recall <- c()
    f1 <- c()
    i <- 1
    
    pred_df <- dt_model$pred
    for (resample in unique(pred_df$Resample)){
      temp <- filter(pred_df, Resample == resample)
      model_cm <- confusionMatrix(temp$pred, temp$obs)
      accuracy[i] <- model_cm$overall[[1]]
      auc_val[[i]] <- auc(roc(as.numeric(temp$pred, ordered = TRUE), as.numeric(temp$obs, ordered = TRUE)))
      sensitivity[[i]] <- model_cm$byClass[[1]]
      specificity[[i]] <- model_cm$byClass[[2]]
      precision[[i]] <- model_cm$byClass[[5]]
      recall[[i]] <- model_cm$byClass[[6]]
      f1[[i]] <- model_cm$byClass[[7]]
      i <- i + 1
    }
  
    accuracy <- mean(accuracy)
    auc_val <- mean(auc_val)
    sensitivity <- mean(sensitivity)
    specificity <- mean(specificity)
    precision <- mean(precision)
    recall <- mean(recall)
    f1 <- mean(f1)
  } 
  else if (grepl("RF", model_name, fixed=TRUE)){
    # Random Forest
    # train model
    dt_model = train(form = target ~ ., data = train_heart, trControl = tr, ntree = as.numeric(str_sub(model_name, start= -2)), method = train_method)
    print(dt_model)
    
    # evaluate model
    model_cm <- confusionMatrix(predict(dt_model, subset(test_heart, select = -c(target))), test_heart$target)
    
    draw_confusion_matrix(model_cm)
    
    print(paste(model_name, 'results'))
    print(model_cm)
    
    # end timer
    elapsed_time <- (proc.time() - ptm)[[3]]
  
    # determine the Accuracy, AUC and as many metrics as returned by the Caret package (confusionMatrix).
    # store results
    accuracy <- model_cm$overall[[1]]
    auc_val <- as.numeric(auc(roc(test_heart$target, factor(predict(dt_model, test_heart), ordered = TRUE))))
    sensitivity <- model_cm$byClass[[1]]
    specificity <- model_cm$byClass[[2]]
    precision <- model_cm$byClass[[5]]
    recall <- model_cm$byClass[[6]]
    f1 <- model_cm$byClass[[7]]
  } 
  else {

    # Cross Validation
    
    dt_model = train(form = target ~ ., data = data, trControl = tr, method = train_method)

    print(dt_model)
    model_cm <- confusionMatrix(dt_model$pred[order(dt_model$pred$rowIndex),]$pred, data$target)
  
    draw_confusion_matrix(model_cm)
  
    print(paste(model_name, 'results'))
    print(model_cm)
  
    # end timer
    elapsed_time <- (proc.time() - ptm)[[3]]

    # determine the Accuracy, AUC and as many metrics as returned by the Caret package (confusionMatrix).
    # store results
    accuracy <- model_cm$overall[[1]]
    auc_val <- as.numeric(auc(roc(test_heart$target, factor(predict(dt_model, test_heart), ordered = TRUE))))
    sensitivity <- model_cm$byClass[[1]]
    specificity <- model_cm$byClass[[2]]
    precision <- model_cm$byClass[[5]]
    recall <- model_cm$byClass[[6]]
    f1 <- model_cm$byClass[[7]]

  }
  
  full_results <- rbind(accuracy, auc_val, sensitivity, specificity, precision, recall, f1, elapsed_time)
  colnames(full_results) <- c(model_name)
  return(full_results)
}
```

Base Metric - Decision Tree

```{r warning=FALSE, message=FALSE}

dt_base <- eval_model("rpart", trainControl(method="none"), "DT Base")
```

Base Metric - SVM 

```{r warning=FALSE, message=FALSE}
svm_base <- eval_model("svmLinearWeights", trainControl(method="none"), "SVM Base")
```

5 Cross Validation Folds - Decision Tree

```{r warning=FALSE, message=FALSE}
dt_5cv <- eval_model("rpart", tr = trainControl(method = "cv", number = 5, savePredictions = 'final'), "DT 5 cv")
```


5 Cross Validation Folds - SVM

```{r}
svm_5cv <- eval_model("svmLinearWeights", tr = trainControl(method = "cv", number = 5, savePredictions = 'final'), "SVM 5 cv")
```


10 Cross Validation Folds - Decision Tree

```{r warning=FALSE, message=FALSE}
dt_10cv <- eval_model("rpart", trainControl(method = "cv", number = 10, savePredictions = 'final'), "DT 10 cv")
```


10 Cross Validation Folds - SVM

```{r}
svm_10cv <- eval_model("svmLinearWeights", trainControl(method = "cv", number = 10, savePredictions = 'final'), "SVM 10 cv")
```

Bootstrap - Decision Trees

```{r warning=FALSE, message=FALSE}
dt_bt <- eval_model("rpart", trainControl(method="boot", number=200, savePredictions = 'final', returnResamp = 'final'), "DT Bootstrap")
print(dt_bt)
```


Bootstrap - SVM

```{r warning=FALSE, message=FALSE}
set.seed(43)

# 
svm_bt <- eval_model("svmLinearWeights", trainControl(method="boot", number=200, savePredictions = 'final', returnResamp = 'final'), "SVM Bootstrap")
print(svm_bt)
```

```{r warning=FALSE, message=FALSE}
data.frame(cbind(dt_base, dt_5cv, dt_10cv, dt_bt, svm_base, svm_5cv, svm_10cv, svm_bt))
```


## Part B

For the same dataset, set seed (43) split 80/20.

```{r warning=FALSE, message=FALSE}
# do a 80/20 split 
set.seed(43)
train_ind <- sample(seq_len(nrow(data)), size = floor(0.8 * nrow(data)))
train_heart <- data[ train_ind,]
test_heart  <- data[-train_ind,]
```

Using randomForest grow three different forests varuing  the number of trees atleast three times. Start with seeding andfresh split for each forest. Note down as best as you can development (engineering) cost as well as computing cost(elapsed time) for each run. And compare these results with the experiment in Part A. Submit a pdf and executable script in python or R.

```{r warning=FALSE, message=FALSE}
rf_10 <- eval_model("rf", trainControl(), "RF 10")
```

```{r warning=FALSE, message=FALSE}
rf_50 <- eval_model("rf", trainControl(), "RF 50")
```

```{r warning=FALSE, message=FALSE}
rf_99 <- eval_model("rf", trainControl(), "RF 99")
```

```{r warning=FALSE, message=FALSE}
data.frame(cbind(rf_10, rf_50, rf_99))
```

## Part C

Include a summary of your findings. Which of the two methods bootstrap vs cv do you recommend to your customer? And why? Be elaborate. Including computing costs, engineering costs and model performance. Did you incorporate Pareto's maxim or the Razor and how did these two heuristics influence your decision?

```{r warning=FALSE, message=FALSE}
result <- data.frame(cbind(dt_base, dt_5cv, dt_10cv, dt_bt, svm_base, svm_5cv, svm_10cv, svm_bt, rf_10, rf_50, rf_99))
result %>% kable() %>% kable_styling() %>% scroll_box(width = "800px", height = "400px")
```

### Analysis

I would recommend cross validation. Cross-Validation is a very powerful tool. It helps us better use our data, and it gives us much more information about our algorithm performance. 


SVM

It looks like the base SVM did most of the work as suggested by Pareto principle and cross validation gave it a performance boost. 10-fold did not add any value to the 5-fold. There was a increase in processing time. According to Occam’s razor, we should use the simple model, i.e. 5-fold.

Decision Tree

Here cross validation helped with parameter selection. 5 fold cross validation yields better results than 10 fold cross validation. impler solution (5-fold CV) should be used (Occam’s razor principle). Did not find any added benefit using 10 fold CV.

Random Forest

Random Forest had the same performance as the base SVM model. It took more time to compute. It is better to use simpler model according to Occam’s razor principle.
